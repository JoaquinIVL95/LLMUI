# LLMUI - Sistema de B√∫squeda Web para LLM Local

Sistema de chat inteligente que integra tu LLM local de Ollama con b√∫squeda web en tiempo real, permiti√©ndote programar con informaci√≥n actualizada de internet.

## Modos de Uso

### üåê Servidor Web (Interfaz Gr√°fica)
Interfaz web moderna con chat interactivo y panel de resultados de b√∫squeda.

```bash
npm start
# Abre http://localhost:3000
```

### üíª CLI (L√≠nea de Comandos) - **NUEVO**
Asistente de programaci√≥n con capacidad de ejecutar c√≥digo y manipular archivos.

```bash
npm run cli
```

**¬°El CLI puede ejecutar c√≥digo Python, JavaScript y Bash directamente!**

Ver [CLI_README.md](CLI_README.md) para documentaci√≥n completa del CLI.

## Caracter√≠sticas

### Servidor Web
- ü§ñ **Integraci√≥n con Ollama**: Usa tu modelo LLM local
- üîç **B√∫squeda Web**: B√∫squeda autom√°tica en internet usando SerpAPI
- üí¨ **Chat Interactivo**: Interfaz web moderna y responsive
- üìä **Resultados en Tiempo Real**: Visualiza los resultados de b√∫squeda junto con las respuestas

### CLI (Modo Consola)
- ‚ö° **Ejecuci√≥n de C√≥digo**: JavaScript, Python, Bash
- üìÅ **Manejo de Archivos**: Crear, leer, modificar archivos
- üîç **B√∫squeda Web Integrada**: Informaci√≥n actualizada durante la programaci√≥n
- üéØ **Proyectos Completos**: Crea aplicaciones multi-archivo
- üõ†Ô∏è **Debugging Interactivo**: Prueba y corrige c√≥digo en tiempo real

## Requisitos Previos

1. **Node.js** (versi√≥n 18 o superior)
2. **Ollama** instalado y corriendo
   - Descarga desde: https://ollama.ai
   - Instala un modelo: `ollama pull llama2` (o el modelo que prefieras)

## Instalaci√≥n

1. Instalar dependencias:
```bash
npm install
```

2. Configurar variables de entorno:
```bash
# Copiar el archivo de ejemplo
cp .env.example .env

# Editar .env con tus configuraciones
```

3. Configuraci√≥n del archivo `.env`:
```env
# Host de Ollama (por defecto localhost)
OLLAMA_HOST=http://localhost:11434

# Modelo a usar (cualquier modelo que tengas instalado en Ollama)
OLLAMA_MODEL=llama2

# OPCIONAL: SerpAPI para b√∫squedas m√°s completas
# Si no lo configuras, usar√° DuckDuckGo (gratis, sin l√≠mites)
SERPAPI_KEY=tu_clave_aqui

# Puerto del servidor
PORT=3000
```

## Uso

### Iniciar el servidor

```bash
# Modo normal
npm start

# Modo desarrollo (con auto-reload)
npm run dev
```

El servidor estar√° disponible en: http://localhost:3000

### Verificar que Ollama est√© corriendo

```bash
# En otra terminal
ollama serve
```

### Probar modelos disponibles

```bash
# Ver modelos instalados
ollama list

# Descargar nuevos modelos
ollama pull llama2
ollama pull codellama
ollama pull mistral
```

## API Endpoints

### POST `/api/chat`
Chat con el LLM y b√∫squeda web opcional.

```javascript
// Request
{
  "message": "¬øC√≥mo usar async/await en JavaScript?",
  "useSearch": true  // opcional, default: true
}

// Response
{
  "response": "Respuesta del LLM...",
  "searchResults": [...],  // resultados de b√∫squeda si los hay
  "usedSearch": true
}
```

### POST `/api/search`
B√∫squeda web directa sin el LLM.

```javascript
// Request
{
  "query": "JavaScript async await tutorial"
}

// Response
{
  "results": [
    {
      "title": "...",
      "snippet": "...",
      "url": "..."
    }
  ]
}
```

### GET `/api/status`
Verificar estado de Ollama y modelos disponibles.

```javascript
// Response
{
  "status": "ok",
  "models": [...],
  "currentModel": "llama2"
}
```

## Opciones de B√∫squeda Web

### 1. DuckDuckGo (Gratis)
Por defecto, el sistema usa la API gratuita de DuckDuckGo. No requiere configuraci√≥n adicional.

**Ventajas:**
- Completamente gratis
- Sin l√≠mites de uso
- No requiere API key

**Limitaciones:**
- Resultados m√°s limitados que Google
- Menos contexto en algunos casos

### 2. SerpAPI (Recomendado para uso intensivo)
Para mejores resultados, puedes usar SerpAPI.

1. Reg√≠strate en: https://serpapi.com/
2. Obt√©n tu API key (100 b√∫squedas gratis/mes)
3. Agr√©gala al archivo `.env`:
```env
SERPAPI_KEY=tu_clave_aqui
```

**Ventajas:**
- Resultados de Google
- M√°s precisos y completos
- Mejor para consultas complejas

## Ejemplos de Uso

### Preguntas de Programaci√≥n
```
Usuario: ¬øCu√°l es la √∫ltima versi√≥n de React y sus nuevas caracter√≠sticas?
Sistema: [Busca en web] ‚Üí [Procesa con LLM] ‚Üí Respuesta actualizada
```

### Resolver Errores
```
Usuario: Tengo un error "Cannot read property of undefined" en mi c√≥digo React
Sistema: [Busca soluciones] ‚Üí [Analiza con LLM] ‚Üí Soluci√≥n con ejemplos
```

### Aprender Nuevas Tecnolog√≠as
```
Usuario: ¬øC√≥mo empezar con Next.js 14?
Sistema: [Busca documentaci√≥n] ‚Üí [Genera tutorial] ‚Üí Gu√≠a paso a paso
```

## Troubleshooting

### Error: "No se pudo conectar con Ollama"
- Verifica que Ollama est√© corriendo: `ollama serve`
- Comprueba el puerto en `.env` (default: 11434)

### Error: "Modelo no encontrado"
- Lista modelos disponibles: `ollama list`
- Descarga el modelo: `ollama pull llama2`
- Verifica el nombre en `.env`

### B√∫squedas no funcionan
- DuckDuckGo deber√≠a funcionar sin configuraci√≥n
- Si usas SerpAPI, verifica tu API key y cuota

### El servidor no inicia
- Verifica que el puerto 3000 est√© libre
- Instala dependencias: `npm install`
- Revisa el archivo `.env`

## Modelos Recomendados

Para programaci√≥n:
- **codellama**: Especializado en c√≥digo
- **llama2**: Bueno para prop√≥sito general
- **mistral**: R√°pido y preciso
- **dolphin-mixtral**: Excelente para c√≥digo

Instalar un modelo:
```bash
ollama pull codellama
```

Luego actualizar `.env`:
```env
OLLAMA_MODEL=codellama
```

## Personalizaci√≥n

### Cambiar el prompt del sistema
Edita `server.js` l√≠nea 112:
```javascript
const systemPrompt = `Tu prompt personalizado aqu√≠...`;
```

### Modificar n√∫mero de resultados de b√∫squeda
Edita `server.js` l√≠nea 57 para DuckDuckGo o l√≠nea 84 para SerpAPI:
```javascript
.slice(0, 10)  // Cambiar de 5 a 10 resultados
```

## Tecnolog√≠as Utilizadas

- **Backend**: Node.js, Express
- **LLM**: Ollama
- **B√∫squeda**: DuckDuckGo API / SerpAPI
- **Frontend**: HTML, CSS, JavaScript vanilla

## Contribuir

¬°Las contribuciones son bienvenidas! Por favor:
1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## Licencia

MIT

## Soporte

Si encuentras problemas:
1. Revisa la secci√≥n de Troubleshooting
2. Verifica que Ollama est√© corriendo
3. Comprueba los logs del servidor
4. Abre un issue en GitHub

---

¬°Disfruta programando con tu asistente LLM potenciado con b√∫squeda web!
